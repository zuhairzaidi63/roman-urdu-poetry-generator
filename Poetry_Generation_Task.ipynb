{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb7_Vc0kheYO",
        "outputId": "b25763da-dabc-4ce6-ed79-c45d20bf06f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the uploaded ZIP file (It will be in /content)\n",
        "zip_path = 'dataset.zip'  # Replace with your uploaded file name\n",
        "extract_path = 'dataset'  # Directory to extract to\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBJoKIPmjQp"
      },
      "source": [
        "Verify the Dataset Folder Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoWrOqDImkbc",
        "outputId": "0b470b69-5e61-411b-fe3f-64db8c9c3e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poets Folder: ['dagh-dehlvi', 'jigar-moradabadi', 'sahir-ludhianvi', 'naseer-turabi', 'kaifi-azmi', 'firaq-gorakhpuri', 'ameer-khusrau', 'parveen-shakir', 'javed-akhtar', 'nazm-tabatabai', 'ahmad-faraz', 'meer-anees', 'jaan-nisar-akhtar', 'waseem-barelvi', 'altaf-hussain-hali', 'mirza-ghalib', 'allama-iqbal', 'nida-fazli', 'fahmida-riaz', 'naji-shakir', 'gulzar', 'wali-mohammad-wali', 'jaun-eliya', 'meer-taqi-meer', 'mohsin-naqvi', 'bahadur-shah-zafar', 'akbar-allahabadi', '.DS_Store', 'noon-meem-rashid', 'faiz-ahmad-faiz', 'habib-jalib']\n",
            "Subfolders for dagh-dehlvi: ['hi', 'en', 'ur']\n"
          ]
        }
      ],
      "source": [
        "# Update the dataset directory path\n",
        "dataset_dir = 'dataset/dataset'  # Adjusted path\n",
        "\n",
        "# List the poets' folders\n",
        "print(\"Poets Folder:\", os.listdir(dataset_dir))\n",
        "\n",
        "# Pick the first poet's folder\n",
        "sample_poet = os.listdir(dataset_dir)[0]\n",
        "sample_poet_path = os.path.join(dataset_dir, sample_poet)\n",
        "\n",
        "# List the subfolders inside the poet's folder (should be 'en', 'ur', 'hi')\n",
        "print(f\"Subfolders for {sample_poet}: {os.listdir(sample_poet_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3vDeNU2oq5h"
      },
      "source": [
        "Load & Read the Poetry Files from \"en\" Folders\n",
        "Now, we need to extract poetry only from the \"en\" folder of each poet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5wrkcBvorhU",
        "outputId": "11e66df7-c123-4a2e-8166-5ba2cf1c4600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Poems Collected: 1314\n",
            "Sample Poem:\n",
            " \n",
            "tum ā.īna hī na har baar dekhte jaao \n",
            "mirī taraf bhī to sarkār dekhte jaao \n",
            "na jaao hāl-e-dil-e-zār dekhte jaao \n",
            "ki jī na chāhe to nā-chār dekhte jaao \n",
            "bahār-e-umr meñ bāġh-e-jahāñ kī sair karo \n",
            "khilā huā hai ye gulzār dekhte jaao \n",
            "yahī to chashm-e-haqīqat nigar kā surma hai \n",
            "nizā-e-kāfir-o-dīñ-dār dekhte jaao \n",
            "uThāo aañkh na sharmāo ye to mahfil hai \n",
            "ġhazab se jānib-e-aġhyār dekhte jaao \n",
            "nahīñ hai jins-e-vafā kī tumheñ jo qadr na ho \n",
            "baneñge kitne ḳharīdār dekhte jaao \n",
            "tumheñ ġharaz jo karo ra\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to the dataset containing poet folders\n",
        "dataset_dir = 'dataset/dataset'\n",
        "\n",
        "# List to store all poetry texts\n",
        "poetry_texts = []\n",
        "\n",
        "# Iterate through each poet's folder\n",
        "for poet in os.listdir(dataset_dir):\n",
        "    poet_path = os.path.join(dataset_dir, poet)\n",
        "\n",
        "    # Check if it's a directory (ignore files like .DS_Store)\n",
        "    if os.path.isdir(poet_path):\n",
        "        en_path = os.path.join(poet_path, \"en\")  # Path to \"en\" poetry folder\n",
        "\n",
        "        # Check if \"en\" folder exists\n",
        "        if os.path.exists(en_path):\n",
        "            for file in os.listdir(en_path):\n",
        "                file_path = os.path.join(en_path, file)\n",
        "\n",
        "                # Read the poetry file and append to list\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    poetry_texts.append(f.read())\n",
        "\n",
        "# Print sample poetry to verify\n",
        "print(f\"Total Poems Collected: {len(poetry_texts)}\")\n",
        "print(\"Sample Poem:\\n\", poetry_texts[0][:500])  # Print first 500 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygkcb1IppS1a"
      },
      "source": [
        "Preprocess the Text Data\n",
        "Before feeding data into the model, we need to:\n",
        "\n",
        "Lowercase the text (optional)\n",
        "Remove unwanted characters & extra spaces\n",
        "Tokenize the text into sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzPO9DGlo50T",
        "outputId": "c8a5ca15-fdad-479f-8512-a0760dc281c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Sample:\n",
            " \n",
            "tum ā.īna hī na har baar dekhte jaao \n",
            "mirī taraf bhī to sarkār dekhte jaao \n",
            "na jaao hāl-e-dil-e-zār dekhte jaao \n",
            "ki jī na chāhe to nā-chār dekhte jaao \n",
            "bahār-e-umr meñ bāġh-e-jahāñ kī sair karo \n",
            "khilā huā hai ye gulzār dekhte jaao \n",
            "yahī to chashm-e-haqīqat nigar kā surma hai \n",
            "nizā-e-kāfir-o-dīñ-dār dekhte jaao \n",
            "uThāo aañkh na sharmāo ye to mahfil hai \n",
            "ġhazab se jānib-e-aġhyār dekhte jaao \n",
            "nahīñ hai jins-e-vafā kī tumheñ jo qadr na ho \n",
            "baneñge kitne ḳharīdār dekhte jaao \n",
            "tumheñ ġharaz jo karo ra\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters except basic punctuations\n",
        "    # text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\s]', '', text)\n",
        "\n",
        "    # Convert multiple spaces into a single space\n",
        "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to all poems\n",
        "cleaned_poetry = [preprocess_text(poem) for poem in poetry_texts]\n",
        "\n",
        "# Print a cleaned sample\n",
        "print(\"Cleaned Sample:\\n\", cleaned_poetry[0][:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Z3AVZnqBzc"
      },
      "source": [
        "Convert Poetry into Sequences\n",
        "Since LSTM models require numerical input, we need to:\n",
        "\n",
        "Tokenize words into unique integers\n",
        "Create input-output sequences (X: previous words, Y: next word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI-Op1nuqCy9",
        "outputId": "7d1e5954-ecf6-43fb-f49e-c2b74de73146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.4)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m235.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m258.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m237.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m237.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m318.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m180.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m173.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m194.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m251.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m307.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m276.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m241.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m251.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m320.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2 --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_in_BdBHppwR",
        "outputId": "8edf2463-b0bc-403c-e274-ac23862d60d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 17345\n",
            "Encoded Sample Poem: [2, 52, 410, 29, 9, 38, 166, 164, 325, 2, 83, 158, 11, 12, 3903, 164, 325, 2, 9, 325]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Tokenize poetry into words\n",
        "def tokenize_poetry(poetry_list):\n",
        "    tokenized_poems = []\n",
        "    for poem in poetry_list:\n",
        "        # Replace actual newlines with a special token\n",
        "        poem = poem.replace(\"\\n\", \" <NEWLINE> \")\n",
        "        tokenized_poems.append(poem.split())  # Tokenize normally\n",
        "    return tokenized_poems\n",
        "\n",
        "\n",
        "# Build vocabulary\n",
        "tokenized_poems = tokenize_poetry(cleaned_poetry)\n",
        "\n",
        "vocab = build_vocab_from_iterator(tokenized_poems, specials=[\"<PAD>\", \"<UNK>\", \"<NEWLINE>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "\n",
        "# Convert words to indices\n",
        "def encode_poetry(poem):\n",
        "    return [vocab[word] for word in poem]\n",
        "\n",
        "encoded_poems = [encode_poetry(poem) for poem in tokenized_poems]\n",
        "\n",
        "# Print vocabulary size\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n",
        "print(\"Encoded Sample Poem:\", encoded_poems[0][:20])  # Show first 10 encoded words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVocab:\n",
        "    def __init__(self, vocab):\n",
        "        self.stoi = vocab.get_stoi()  # Word to index mapping\n",
        "        self.itos = vocab.get_itos()  # Index to word mapping\n",
        "        self.default_index = vocab.get_default_index()  # Default index for unknown words\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        return self.stoi.get(word, self.default_index)\n",
        "\n",
        "    def lookup_token(self, index):\n",
        "        return self.itos[index] if 0 <= index < len(self.itos) else \"<UNK>\""
      ],
      "metadata": {
        "id": "WyGzfzgYOS0P"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "simple_vocab = SimpleVocab(vocab)\n",
        "with open('simple_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(simple_vocab, f)\n"
      ],
      "metadata": {
        "id": "nTdAa6ITOo5X"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpIzaC3nsiRl"
      },
      "source": [
        "Prepare Data for Training\n",
        "We need to:\n",
        "\n",
        "Create sequences of words (e.g., first 5 words → predict next word)\n",
        "Pad sequences for uniform input size\n",
        "Split data into training & validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OunMnNxAsi9B",
        "outputId": "f340c7ff-3064-4bbe-a9a8-e37b10b98e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Input: tensor([ 44,   8, 901,  12,  32, 600])\n",
            "Sample Target: tensor(9)\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, poems, seq_length=6):\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "\n",
        "        # Create sequences\n",
        "        for poem in poems:\n",
        "            if len(poem) > seq_length:\n",
        "                for i in range(len(poem) - seq_length):\n",
        "                    seq = poem[i:i+seq_length]\n",
        "                    target = poem[i+seq_length]\n",
        "                    self.data.append((seq, target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence, target = self.data[idx]\n",
        "        return torch.tensor(sequence), torch.tensor(target)\n",
        "\n",
        "# Create dataset\n",
        "seq_length = 6\n",
        "dataset = PoetryDataset(encoded_poems, seq_length)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Print sample batch\n",
        "for x, y in dataloader:\n",
        "    print(\"Sample Input:\", x[0])\n",
        "    print(\"Sample Target:\", y[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVnYSbXFsvrX"
      },
      "source": [
        "Build an LSTM Model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgxDVxyYswTz",
        "outputId": "13df1af6-4046-4599-801b-914989f88653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Initialized\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PoetryLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(PoetryLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Layer Normalization (Added)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Apply Layer Normalization\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "\n",
        "        out = self.fc(lstm_out[:, -1])  # Use last LSTM output\n",
        "        return out\n",
        "\n",
        "# Model\n",
        "model = PoetryLSTM(vocab_size=len(vocab)).to(\"cuda\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Learning Rate Scheduler (Reduce LR after 25 epochs)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
        "\n",
        "print(\"Model Initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKsms8lPs4P2"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuYeXzFs4w2",
        "outputId": "e34a605e-64db-43cf-e531-388ef6dec22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/51], Loss: 6.3095, LR: 0.000500\n",
            "Epoch [2/51], Loss: 5.5091, LR: 0.000500\n",
            "Epoch [3/51], Loss: 4.8910, LR: 0.000500\n",
            "Epoch [4/51], Loss: 4.2155, LR: 0.000500\n",
            "Epoch [5/51], Loss: 3.4983, LR: 0.000500\n",
            "Epoch [6/51], Loss: 2.8309, LR: 0.000500\n",
            "Epoch [7/51], Loss: 2.2810, LR: 0.000500\n",
            "Epoch [8/51], Loss: 1.8264, LR: 0.000500\n",
            "Epoch [9/51], Loss: 1.4493, LR: 0.000500\n",
            "Epoch [10/51], Loss: 1.1344, LR: 0.000500\n",
            "Epoch [11/51], Loss: 0.8883, LR: 0.000500\n",
            "Epoch [12/51], Loss: 0.6999, LR: 0.000500\n",
            "Epoch [13/51], Loss: 0.5694, LR: 0.000500\n",
            "Epoch [14/51], Loss: 0.4815, LR: 0.000500\n",
            "Epoch [15/51], Loss: 0.4266, LR: 0.000500\n",
            "Epoch [16/51], Loss: 0.3895, LR: 0.000500\n",
            "Epoch [17/51], Loss: 0.3618, LR: 0.000500\n",
            "Epoch [18/51], Loss: 0.3367, LR: 0.000500\n",
            "Epoch [19/51], Loss: 0.3198, LR: 0.000500\n",
            "Epoch [20/51], Loss: 0.3062, LR: 0.000500\n",
            "Epoch [21/51], Loss: 0.2939, LR: 0.000500\n",
            "Epoch [22/51], Loss: 0.2857, LR: 0.000500\n",
            "Epoch [23/51], Loss: 0.2744, LR: 0.000500\n",
            "Epoch [24/51], Loss: 0.2669, LR: 0.000500\n",
            "Epoch [25/51], Loss: 0.2580, LR: 0.000050\n",
            "Epoch [26/51], Loss: 0.0970, LR: 0.000050\n",
            "Epoch [27/51], Loss: 0.0236, LR: 0.000050\n",
            "Epoch [28/51], Loss: 0.0176, LR: 0.000050\n",
            "Epoch [29/51], Loss: 0.0161, LR: 0.000050\n",
            "Epoch [30/51], Loss: 0.0155, LR: 0.000050\n",
            "Epoch [31/51], Loss: 0.0137, LR: 0.000050\n",
            "Epoch [32/51], Loss: 0.0136, LR: 0.000050\n",
            "Epoch [33/51], Loss: 0.0125, LR: 0.000050\n",
            "Epoch [34/51], Loss: 0.0126, LR: 0.000050\n",
            "Epoch [35/51], Loss: 0.0122, LR: 0.000050\n",
            "Epoch [36/51], Loss: 0.0118, LR: 0.000050\n",
            "Epoch [37/51], Loss: 0.0113, LR: 0.000050\n",
            "Epoch [38/51], Loss: 0.0111, LR: 0.000050\n",
            "Epoch [39/51], Loss: 0.0110, LR: 0.000050\n",
            "Epoch [40/51], Loss: 0.0107, LR: 0.000050\n",
            "Epoch [41/51], Loss: 0.0106, LR: 0.000050\n",
            "Epoch [42/51], Loss: 0.0102, LR: 0.000050\n",
            "Epoch [43/51], Loss: 0.0104, LR: 0.000050\n",
            "Epoch [44/51], Loss: 0.0102, LR: 0.000050\n",
            "Epoch [45/51], Loss: 0.0097, LR: 0.000050\n",
            "Epoch [46/51], Loss: 0.0101, LR: 0.000050\n",
            "Epoch [47/51], Loss: 0.0099, LR: 0.000050\n",
            "Epoch [48/51], Loss: 0.0097, LR: 0.000050\n",
            "Epoch [49/51], Loss: 0.0095, LR: 0.000050\n",
            "Epoch [50/51], Loss: 0.0098, LR: 0.000005\n",
            "Epoch [51/51], Loss: 0.0061, LR: 0.000005\n",
            "Training Completed!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 51\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        x_batch, y_batch = x_batch.to(\"cuda\"), y_batch.to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.detach().item()  # Detach to save memory\n",
        "\n",
        "    # Reduce learning rate after step_size epochs\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(\"Training Completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvn3xg7Rs647",
        "outputId": "6e2386a8-25ac-404b-bed3-3d27ea54a59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tum ā.īna hī na har baar dekhte jaao\n",
            "mirī taraf bhī to sarkār dekhte jaao\n",
            "na jaao hāl-e-dil-e-zār dekhte jaao\n",
            "ki jī na chāhe to nā-chār dekhte jaao\n",
            "bahār-e-umr meñ bāġh-e-jahāñ kī sair karo\n",
            "khilā huā hai ye gulzār dekhte jaao\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_poetry(seed_text, model, vocab, max_words=40):\n",
        "    model.eval()\n",
        "    words = seed_text.split()\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        encoded = torch.tensor([vocab[word] for word in words[-6:]]).unsqueeze(0).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            output = model(encoded)\n",
        "            next_word = vocab.lookup_token(output.argmax().item())\n",
        "            words.append(next_word)\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Example Usage\n",
        "seed = \"tum ā.īna hī na har baar\"\n",
        "def print_poetry(generated_text):\n",
        "    formatted_text = generated_text.replace(\" <NEWLINE> \", \"\\n\")\n",
        "    return formatted_text\n",
        "\n",
        "generate=generate_poetry(seed, model, vocab)\n",
        "print(print_poetry(generate))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "hGJDYHowb6br"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"poetry_lstm_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3o7nicltnNe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1ed97ad0-61a4-41f6-cfb1-0515d9448f56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be4376d5-b4eb-4dd0-a287-9ab8b6838763\", \"poetry_lstm_model.pth\", 30402727)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/poetry_lstm_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iT03JhKjes-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}